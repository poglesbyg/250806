# ğŸ”¥ PyTorch Showcase

A comprehensive demonstration of PyTorch capabilities featuring modern deep learning architectures and techniques.

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸŒŸ Features

This project showcases three major areas of deep learning:

### ğŸ–¼ï¸ Computer Vision - CNN
- **ResNet-inspired CNN** for CIFAR-10 classification
- Residual connections and batch normalization
- Data augmentation and mixed precision training
- Feature map and filter visualization
- Grad-CAM for explainable AI

### ğŸ¤– Natural Language Processing - Transformer
- **Custom Transformer** architecture for text classification
- Multi-head self-attention mechanism
- Positional encoding and layer normalization
- Attention weight visualization
- Gradient clipping and learning rate scheduling

### ğŸ¨ Generative AI - GAN
- **DCGAN** for MNIST image generation
- Generator and discriminator networks
- Adversarial training with spectral normalization
- Progressive sample generation
- Latent space interpolation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd pytorch-showcase
```

2. Install dependencies using uv:
```bash
uv sync
```

### Running the Demos

#### Run All Demonstrations
```bash
uv run python main.py --all
```

#### Run Individual Demos
```bash
# CNN Demo (CIFAR-10 Classification)
uv run python main.py --cnn

# Transformer Demo (Text Classification)  
uv run python main.py --transformer

# GAN Demo (MNIST Generation)
uv run python main.py --gan
```

#### Custom Parameters
```bash
# Run CNN with custom settings
uv run python main.py --cnn --epochs 30 --batch-size 64 --learning-rate 0.01

# Run Transformer with large model
uv run python main.py --transformer --model-size large --epochs 20

# Run conditional GAN
uv run python main.py --gan --gan-type conditional --epochs 100
```

## ğŸ“Š What You'll Get

After running the demos, you'll find comprehensive outputs in the `./outputs/` directory:

### CNN Results
- **Training curves**: Loss and accuracy over epochs
- **Confusion matrix**: Detailed classification performance
- **Feature maps**: Visualization of learned features
- **Filters**: Convolutional filter visualizations
- **Grad-CAM**: Attention heatmaps for explainability

### Transformer Results
- **Training progress**: Loss and metrics tracking
- **Attention weights**: Multi-head attention visualizations
- **Sample predictions**: Model predictions on test examples
- **Confusion matrix**: Classification performance analysis

### GAN Results
- **Generated samples**: High-quality generated images
- **Training dynamics**: Generator vs discriminator loss curves
- **Latent interpolation**: Smooth transitions in latent space
- **Sample diversity**: Quality assessment of generated images
- **Conditional samples** (if using conditional GAN)

## ğŸ—ï¸ Project Structure

```
pytorch-showcase/
â”œâ”€â”€ pytorch_showcase/
â”‚   â”œâ”€â”€ models/                 # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ cnn.py             # ResNet-inspired CNN
â”‚   â”‚   â”œâ”€â”€ transformer.py     # Custom transformer
â”‚   â”‚   â””â”€â”€ gan.py             # DCGAN implementation
â”‚   â”œâ”€â”€ utils/                 # Utilities and tools
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loops
â”‚   â”‚   â”œâ”€â”€ data_utils.py      # Data loading
â”‚   â”‚   â”œâ”€â”€ metrics.py         # Evaluation metrics
â”‚   â”‚   â””â”€â”€ visualizer.py      # Visualization tools
â”‚   â””â”€â”€ demos/                 # Demo scripts
â”‚       â”œâ”€â”€ cnn_demo.py
â”‚       â”œâ”€â”€ transformer_demo.py
â”‚       â””â”€â”€ gan_demo.py
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ pyproject.toml            # Project dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Advanced Usage

### Custom Training

You can import and use the models directly:

```python
from pytorch_showcase.models import CIFAR10CNN, TextClassifier, SimpleGAN
from pytorch_showcase.utils import Trainer, get_cifar10_loaders

# Create and train a custom CNN
model = CIFAR10CNN(num_classes=10)
train_loader, val_loader, test_loader = get_cifar10_loaders()

trainer = Trainer(model, criterion, optimizer, device)
results = trainer.fit(train_loader, val_loader, epochs=50)
```

### Jupyter Notebooks

Interactive notebooks are available in the `notebooks/` directory:
- `cnn_exploration.ipynb`: Deep dive into CNN architectures
- `transformer_attention.ipynb`: Attention mechanism analysis
- `gan_latent_space.ipynb`: Exploring GAN latent spaces

### Weights & Biases Integration

Enable experiment tracking with W&B:

```bash
uv run python main.py --all --wandb
```

## ğŸ¯ Key Learning Objectives

This project demonstrates:

1. **Modern PyTorch Patterns**
   - Custom datasets and data loaders
   - Mixed precision training
   - Learning rate scheduling
   - Gradient clipping

2. **Model Architecture Design**
   - Residual connections
   - Multi-head attention
   - Adversarial training
   - Batch/layer normalization

3. **Training Best Practices**
   - Data augmentation
   - Early stopping
   - Checkpointing
   - Hyperparameter tuning

4. **Evaluation and Visualization**
   - Comprehensive metrics
   - Feature visualization
   - Attention analysis
   - Generated sample quality

## ğŸ§ª Experimental Features

### Model Variants
- **CNN**: Different depths and widths
- **Transformer**: Small, base, and large configurations
- **GAN**: Simple and conditional variants

### Advanced Techniques
- Spectral normalization for GANs
- Gradient penalty (WGAN-GP)
- MixUp and CutMix augmentation
- OneCycle learning rate scheduling

## ğŸ“ˆ Performance Benchmarks

Typical results on default settings:

| Model | Dataset | Metric | Score |
|-------|---------|--------|-------|
| CNN | CIFAR-10 | Accuracy | ~85-90% |
| Transformer | Text Classification | Accuracy | ~80-85% |
| GAN | MNIST Generation | FID | ~20-30 |

*Results may vary based on hardware and random initialization*

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Additional model architectures (Vision Transformer, BERT, StyleGAN)
- More datasets and benchmarks
- Advanced training techniques
- Mobile/edge deployment examples

## ğŸ“š Educational Resources

This project serves as a practical companion to:
- Deep Learning courses
- PyTorch tutorials
- Computer vision and NLP research
- Machine learning engineering practices

## ğŸ› Troubleshooting

### Common Issues

**CUDA out of memory**:
```bash
# Reduce batch size
uv run python main.py --cnn --batch-size 32

# Use CPU
uv run python main.py --all --device cpu
```

**Slow training**:
- Ensure CUDA is available and properly installed
- Consider using smaller model sizes
- Reduce number of epochs for quick testing

**Import errors**:
```bash
# Reinstall dependencies
uv sync --force
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent framework
- Research papers that inspired the architectures
- Open source community for tools and libraries

---

**Happy Learning! ğŸ“**

*This project is designed for educational purposes to showcase PyTorch capabilities and modern deep learning techniques.*